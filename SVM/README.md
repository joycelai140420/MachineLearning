Support Vector Machine，简称 SVM

基本原理

SVM 的基本思想是在特征空间中寻找一个超平面，使得两个类别的数据点之间的间隔（margin）最大化。这个最大化间隔的想法是基于这样一个观察：间隔越大，分类器在未见数据上的泛化能力通常也越强。
在二维空间中，一个超平面可以被理解为一条直线。在更高维度的空间中，这个超平面就成了可以将数据分割成两部分的 n-1 维的平面。SVM 模型主要通过解决一个优化问题来寻找这个超平面。

核技巧

SVM 的一个重要特性是利用核技巧（kernel trick），通过核函数将输入空间映射到一个更高维的特征空间，使得在新的特征空间中，原本线性不可分的数据变得线性可分。常用的核函数包括线性核、多项式核、径向基函数核（RBF，也称高斯核）和sigmoid核。

优点

    有效性： 在中等大小的数据集上，尤其是在数据维度较高时，SVM 通常能提供较好的结果。
    泛化能力： 由于其决策边界是由数据点中最难分辨的点（支持向量）决定的，因此 SVM 往往具有较好的泛化能力。
    适用范围广： 通过选择合适的核函数，可以解决各种类型的问题（线性/非线性）。


缺点

    可扩展性： 对于大规模数据集，SVM 的训练时间可能会非常长，这限制了其在大数据时代的应用。
    参数调整： SVM 的性能在很大程度上依赖于核函数的选择和参数设置（如 C 和核参数）。不恰当的选择可能导致模型过拟合或欠拟合。
    结果解释性： SVM 模型的结果比较难以解释，尤其是使用了非线性核函数时，决策函数和特征空间的关系不直观。

应用领域

    生物信息学： 用于蛋白质分类、癌症分类等。
    图像处理： 用于图像分类和手写数字识别。
    文本分类： 例如垃圾邮件检测和新闻文章分类。
    金融： 用于信用评分和股市分析。


![image](https://github.com/joycelai140420/MachineLearning/assets/167413809/6ccd4698-d84d-4e84-912d-133d8a7b57b9)




