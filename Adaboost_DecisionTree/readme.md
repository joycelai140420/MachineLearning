AdaBoost算法简介

AdaBoost，全称为Adaptive Boosting（自适应增强），是一种将多个弱学习器组合成强学习器的技术。 算法的核心思想是针对同一个训练集训练不同的弱学习器（表现比较差的算法），通常是简单的决策树。 每一轮训练之后，AdaBoost会增加被前一个分类器错误分类的样本的权重，而正确分类的样本权重会减少。 然后，基于更新过的权重来训练下一个分类器。每个分类器根据其准确性赋予一定的权重，最终所有分类器通过加权投票或求和来进行最终的决策。

这种方法使得模型在每一轮都聚焦于难以分类的样本，逐渐提升整体的分类性能。AdaBoost是一个强大的分类技术，尤其是在结合了多个简单模型后，在多种问题上表现出了很高的精度和鲁棒性。

AdaBoost + 决策树组合 
AdaBoost（自适应增强）是一种集成学习方法，经常与决策树结合使用，以提高模型的预测性能。以下是这种组合的一些优点、缺点以及应用领域：

优点： 提高准确率：通过组合多个决策树，AdaBoost帮助减少模型的偏差，通常比单独的决策树具有更高的准确率。 防止过拟合：虽然决策树容易过拟合，但在AdaBoost的框架下，通过逐步添加决策树并调整权重，可以有效控制过拟合。 自动特征选择：AdaBoost结合决策树在训练过程中能够识别重要的数据特征，这样可以提高模型的解释性和效率。 缺点： 敏感于噪声和异常值：AdaBoost调整误分类数据的权重，因此噪声和异常值可能会对最终模型的性能产生不利影响。 计算成本：与单一模型相比，训练多个决策树需要更多的计算资源和时间。 参数调整：需要合理选择弱学习器的数量和深度，这可能需要通过交叉验证等方式进行多次尝试，以找到最佳的参数设置。 应用： AdaBoost与决策树的组合广泛应用于分类问题，如客户流失预测、信用评分、医学诊断以及图像识别等领域。此外，它们也常用于二分类或多分类问题。

代码就利用Adaboost+DecisionTree 针对初音图片进行剪影的分类

