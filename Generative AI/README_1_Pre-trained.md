Generative AI 简介

    Generative AI（生成性人工智能）是一类人工智能技术，主要用于生成新的内容，包括文本、图像、音乐、语音等。

原理

    Generative AI通常基于机器学习模型，特别是生成模型。这些模型学习数据集的分布特性，以便生成新的、与训练数据相似的实例。
    简单来说就是让电脑学会脑补。

常见的生成模型包括
  
    生成对抗网络（GANs）：通过两个网络（生成器和鉴别器）的对抗过程进行学习。生成器生成尽可能真实的数据，而鉴别器尝试区分真实数据和生成器产生的数据。

    变分自编码器（VAEs）：使用概率编码的方法生成新的数据点，其通过编码输入数据到一个潜在空间，然后从这个潜在空间解码来生成数据。

    自回归模型：如PixelRNN，通过之前的数据点预测接下来的数据点。

    Transformer模型：依赖于自注意力机制，适用于处理序列数据，如文本和音频。GPT（生成预训练Transformer）和BERT就是此类模型的例子。

优点

    创新性和多样性：Generative AI可以创造全新的、多样化的内容，推动艺术和设计的界限。

    效率和自动化：在内容生成、模拟和其他多个领域提高效率，减少人工工作量。

    个性化内容：可以生成针对个人偏好和需求定制的内容，如个性化营销、推荐系统等。

缺点

    可控性问题：生成的内容可能难以预测和控制，尤其是在复杂的生成任务中。

    伦理和安全问题：包括生成虚假信息（如深度伪造）、侵犯版权和隐私问题等。

    质量波动：尤其在早期阶段，生成内容的质量可能不稳定，需要大量调整和优化。

Generative AI 这里就先以ChatGPT 开始作为一个方向，以下是来自于台大Hung-yi Lee老师的内容

ChatGPT ，G就是Generative，P就是Pre-trained，T就是Transformer。可以看出核心的几个方向的技术，而Pre-trained又称预训练、也称自督导式学习（Self-supervised Learning ，又叫基石模型（Foundation Model）。预训练又称是自督导式学习，就是人类提供大量的资料让机器学习。机器会透过一些方法，让资料形成成对样子，这种学习方式就称自督导式学习。而ChatGPT是由GPT产生出来的，所以叫做基石模型。所以预训练跟基石模型讲的是同一件事。

我们知道ChatGPT其实就是文字接龙，预训练就是上网获取大量资料，透过这些资料获得知识的过程。再透过继续学习（文献上称微调 finetune）。

督导式学习就是避免学到不好的讯息，要人类老师指导什么是错的什么是对的，一种大方向对错的引导。就例如刚出生的小孩，我们必须先教育他不能摸插头，不能吃大便，手怎么拿筷子，所以通常会有某些偏向性产生，毕竟每一个家庭教出来的小孩，家教都会有些不一样。

![1715219446800](https://github.com/joycelai140420/MachineLearning/assets/167413809/7f5a0590-b92d-40e6-a4c2-054948bf3f95)

预训练可能带来的帮助是举一反三的能力。实验证明在多种语言上做预训练，只要教某一个语言的某一个任务，自动学会其他语言的同样任务。BERT只是一种其中一个大语言模型之一。以此做范例。

![1715569441217](https://github.com/joycelai140420/MachineLearning/assets/167413809/b9366330-d063-4963-ad35-c10f2d5cf3d8)

真正实验数据显示，这是用DRCD一个中文机器阅读理解数据集（质量挺好的繁体中文数据集），让机器做的是中文阅读理解测验。他是要回答中文的问题，读中文的文章，问中问的问题，给出中文的回答。如果没做预训练，只给他一些中文的QA做微调，问中文的问题，假设我们这边看F1就是准确率可以看到中文回答的答案是78.1。然后用中文做预训练，用中文的QA做微调，F1可以提升到89.1。再来如果用104种语言做预训练，分别用中文或英文做微调，你会发现准确率都比没做预训练的高。当然这里人类测试的结果准确率是93.3%。

![1715570348333](https://github.com/joycelai140420/MachineLearning/assets/167413809/8c6afe7a-138f-464e-be9c-89361059dffe)

现在人们对于大型语言模型有两个方向的期待，老师姑且这样分类，一个是期待一成为主专才，专才就是专注于某个领域发光发热，另一个就是成为通才，有就是上知天文地理，下知鸡毛蒜皮。这两种期待导致两种不同类型的使用与大型语言模型的方式。

那么在这里我们先从专才的这边讲起。

BERT这个模型就是一个做文字填空的模型。通常我们使用它的时候，都是在期待一的情境下使用它。在他成为各种专才的时候，我们进行改造，改造有一种是添加外挂，另一种就是对他的参数做一些微调，才能变成某个任务的专才。

微调Finetune，就是有人输入good morning ，你就要输出早安。然后你就要微调语言模型里面内部的参数，让他可以变成一个翻译的专才，而这边的Finetune就是做gradient descent，将语言模型原来的参数当做训练的初始化参数，当做训练的inirialization(像之前做dnn初始化的参数都是随机)。有人输入good morning ，它才可以输出早安。因为bert本身劣势就是讲不出一句话，所以要加外挂才能讲的一句完整的话。而外挂需要另外训练出来的。需要一些标注的资料，才能够将外挂训练出来。

还有一个外挂的技术就是Adapter，就是语言模型我们都不动他，而外插入其他模组，在大型语言模型里面插入额外的插件，这个插件就是新增加一个layer等等。我们只要动Adapter里面参数。

这个Adapter插件，你可以看左下脚连接，里面有各式各样的插件。这边就是列了几种插件常用的插法。

Bitfit：
        把所有bias当做额外插件，做finetune时候只finetune那些neuron的bias，weight都不去动他。

Houlsby:
        虚线是一个transformer encoder的layer。Houlsby就在feed-forward network前面。

AdapterBias：
        是一个与feed-forward network模组平行的，他会对bitfit forward 的output做一些修改，把Bitfit forward的output做一下平移。

Prefix:
        他是去改attention

Lora:
        跟Diffusion model做结合，也是去改attention。在NLP表现很好，在语音上就表现不好。


![1715582069343](https://github.com/joycelai140420/MachineLearning/assets/167413809/0e04eea7-c622-44f2-ad50-32e333b9530f)

总之这些插件都要自已去试试看，如果有Adapter，可以降低自己Finetune做的参数量，只要对Adapter微调就可以，当我们要期待一个机器可以做的事情不止一个是很多个任务，如果不用插件，有100任务，你就要调整100模型形成100全新的模型，100个全新的参数。今天的模型都很大，GPT-3就有176个billion参数，这样一个一个调整是不可能。所以用插件的好处就是语言模型都不动，只对每一个任务的Adapter的查件作微调就可以，到时候要存的参数只有大型语言模型本身（GPT-3），然后每一个任务你只存各个任务的Adapter，一百个任务就存100个Adapter跟一个大型语言模型参数。

![1715583249283](https://github.com/joycelai140420/MachineLearning/assets/167413809/0853d6fd-e1e1-418e-8740-3e317a492c6f)

接下来就介绍一下通才，先看一下机器怎么根据范例来做学习，根据范例来做学习这叫做In-context learning。假设我们语言模型要做某一个任务比如说情感分析，给他一个句子，要他说这个句子是正面还是负面。直接给他句子是没有用，因为就算他有情感分析的能力，也不知道这个任务是要做翻译还是做摘要，还是要干嘛？所以你要告诉他说，现在是做情感分析。
那怎么告诉他要做情感分析呢？就是给他一些句子，例如今天天气真好是正面，今天运气很差是否负面。把这些句子串起来，再加上我感到非常高兴的这句话。统统丢到语言模型里，当做一个文字接龙。然后让语言模型输出，接下来应该输出哪一个词汇，希望他会输出正面，希望他能领悟到要让他做情感分析，输入这个句子，给出正面的回答。

![1715586055299](https://github.com/joycelai140420/MachineLearning/assets/167413809/3dca03fd-5907-4392-a546-8d3a5aa46dc3)

但是真的可以从这个例子学习吗？机器学习我们知道跑gradient descent做一些微调，机器可以学一点东西，但这个不是跑gradient descent。他只是input。所以右上角的paper做一个实验故意给错的答案。一样给他例子里面的标注是错的。
上面是一堆分类任务的正确率平均，下面是一堆多选题的任务正确率的平均。每一块都是一个模型，从左自右是小模型到大模型。蓝色是没有给范例，所以正确率都比较低。黄色是给了范例，然后范例的标注是正确的。红色是给了范例，然后范例的标注是错的。可以看到就算给错的标注正确率没有下降很多。所以给机器范例，它并没有从这些范例做真正的学习。

![1715586138578](https://github.com/joycelai140420/MachineLearning/assets/167413809/77487dff-d491-4c67-820b-dea73fb4e780)

接下来这个作者又做了一个实验。如果我们给了机器一些句子，但这些句子的domainzo非常不一样。这些句子并不是从情感分析的Benchmark Opus拿出来，可能是从WiKipedia上面随便sample来的句子。如果我们给机器奇奇怪怪的句子，结果会怎么样呢？

![1715589470767](https://github.com/joycelai140420/MachineLearning/assets/167413809/a097aed0-240b-4330-a5d5-b2e2088c76a8)

这是实验的结果，黄色是给正确答案，红色给错误答案，紫色是给无关的句子，蓝色是完全没给范例。发现如果这个句子跟你要解的任务是来自于不同的domain，他们内容差异非常大，就会发现紫色这条正确率非常低。所以给机器那些demonstration，那些domain是非常重要。

![1715589545070](https://github.com/joycelai140420/MachineLearning/assets/167413809/6ee1aa40-ce6c-4a2e-81f4-c8f710bc98bb)

所以作者给一个结论就是这些语言模型本来就会做情感分析，你给他这个句子他本来就知道是正面的。他其实不需要这些例子来做学习。而这些例子只是启动它，让他知道他现在要做情感分析。所以例子不用给的多，只是要唤醒机器现在要做什么样的任务。所以在文献上，In-context learning范例的数目并没有这么重要。

但后来google又提出一个paper说，机器是有学到东西，但取决于模型的大小。在下面这个图片上，颜色越深代表代表模型越大，所以由上到下是模型的由大到小，横轴是今天我们在做In-context learning的时候，给机器的例子里面有百分之多少是错误，从25%到100%有百分之多少是错的。灰色这个虚线是random的结果。

从这个图上看发现，给的越多错误的例子，在大模型上正确率下降的幅度比小模型多。所以机器还是有学到东西。尤其是低于灰色的线，必随便猜还低，就更明确的看出，机器不只是没有乱猜，他还是从错误的资料学到错误的讯息进行学习。

最重要是这些语言模型是没有做过预训练的In-context learning就直接拿出来用以后的结果，当然做了In-context learning之后会比较好。

![1715590225855](https://github.com/joycelai140420/MachineLearning/assets/167413809/7998ad14-54de-42af-98b1-527de16caf86)

刚才我们讲的是透过范例来学习（感觉还是要去找一些范例很麻烦），更进一步是怎么让机器透过题目目的叙述来学习。希望透过叙述让机器做翻译就做翻译，教你做摘要就做摘要。能不能让机器直接阅读任务的叙述就知道我们要他干嘛。

如果要让机器可以看到任务的指示，就做对应的事情。其实直接使用一个预训练模型是不够的。直接使用一个文字接龙的模型是效果很差的。所以文字接龙的模型是需要经过一些微调。这个动作就叫instruction tuning。做了这个才能够看得懂人类的指令。给个指令说要做翻译酒知道做翻译，给个摘要指令就做摘要。

![1715591572079](https://github.com/joycelai140420/MachineLearning/assets/167413809/34172891-c74e-4741-8464-28eb66a5dd47)

T0就是根据这个概念去研发，这是Hugging Face的paper。T0做的跟前面讲的是一样，给个指令说要做摘要就知道做摘要，给个分析指令就做分析，给个QA指令就做问答，期待在测试的时候，你直接叫他做natural language inference(自然语言推论)。

![1715591722232](https://github.com/joycelai140420/MachineLearning/assets/167413809/9dae0058-33b2-406e-a9e3-4ecd232a0f2a)

另外一个知名的模型叫FLAN。这是google paper。那你要做T0跟FLAN的模型，起手式就是要先手机一大堆自然语言处理的任务。图片是从FLAN截取出来，可以看到它收集了各式各样自然语言处理的任务。比如翻译就有最右边八个不同的dataset。摘要就有11个dataset等等。收集各式各样自然语言处理的任务，还有标注的资料集。

![1715592054833](https://github.com/joycelai140420/MachineLearning/assets/167413809/b0c1f763-701e-4a0a-ae59-cd1d44552db8)

接下来你就需要把这些任务改写成指令。假设现在做的任务是推论「自然语言推论」。所以要改一个前提（左上图），然后给他一个假设，然后他回答这个前提跟假有没有矛盾。
如果在一般的模型，不是instruction-based的模型，那就是读这两个句子（前提与假设），期待他可以得到正确的结果。但现在我们要叫机器做的事，是人类都看懂的指令。所以这边要问的问题是当人类想要叫机器做自然语言推论时，怎么跟机器说话。

接下来右边就是各种不同的说法来跟机器说话。在FLAN这google paper里面提到每一个NLP任务他们都想了10各不同的描述方式，比如说第三个描述方式就是如果我要叫机器做自然语言推论，就会说请读下面的文章，然后决定假设是否可以推论出前提，前提是...，假设是...，选项是...，期待机器可以得到正确的答案。

简单来说，就是你要想办法把自然语言推论这个任务，用人类的语言把它描述出来，然后变成一个dataset，然后去教机器看看大型语言模型可不可以自动学到，看这些指令，就做她该做的事。

![1715593029539](https://github.com/joycelai140420/MachineLearning/assets/167413809/7c00c2d0-cbca-4237-92c5-2663c15aa4d3)

结果是可以的，FLAN这google paper做了三种不同测试任务，包含自然语言推论，Realig Comprehension、Closed-Book QA。

当他的测试任务是自然语言推论时候，训练资料是没有自然语言推论。（如果测试任务是自然语言推论时候，训练资料是有自然语言推论，这样机器是可以看懂你叫他做的事情就是推论的指令。）然后来看看机器在他没看过的指令下，自动知道人要叫他做什么事情。

下图数值是越高越好，黄色是只给他指令，橘色是给他指令也做In-context learning，然后蓝色FLAN是有做instruction-based，所以有拿一堆资料教机器说看到人给这样的指示时候你要做什么。黄色是没看过指令，蓝色有，有学过怎么根据人的指示作出合理的回应。蓝色得到结果是比橘色一样做过In-context learning还要好。所以机器是有机会学会根据人下的指令做事情。他学过的指令可以generaliza到没有看过的指令上。

![1715593546340](https://github.com/joycelai140420/MachineLearning/assets/167413809/a86499c6-7c08-45a3-9297-b303cb83238e)

接下来要讲的另一个技术是Chain of Thought(CoT) Prompting。之前学者发现如果是做In-context learning但在做数学的推论往往效果不好。Chain of Thought(CoT)的技术是，给他问题，也给推论的过程，最后给他答案。期待当遇到新的问题，他会给出推理的过程与答案。发现是有成效的。
下面是Chain of Thought(CoT)实验结果，是做在数学的题目上，白色是微调GPT-3，蓝色是过去做最好的case，黄色是做标准等prompting PaLM（instruction-based给他指令不给他答案），红色是做了Chain of Thought(CoT)的PaLM。

![1715594977526](https://github.com/joycelai140420/MachineLearning/assets/167413809/f0580aaf-377a-4d4b-8e7f-1e18e4f1d586)

有一种变形Chain of Thought(CoT)就是推论过程给他指令是「Let's think step by steps。」效果也非常好。

还有一个招式是Self-consistency，除了叫他 step by steps。还跟他说把每个推论过程跟答案（因为我们知道每次推论的结果跟答案有时候都不会是一样）然后看看哪一个答案出现最多次，那就是正确答案。虽然这个方法有效，但前提是要能让机器给出推论的过程而不是只给答案，因为发现要求语言模型只给答案出现的答案都很一致，如果要机器给出推论过程，往往就会发发现答案会不一样。

![1715595552537](https://github.com/joycelai140420/MachineLearning/assets/167413809/e897f393-08ee-46da-98f0-43d3ef188832)

接下来如何让机器找Prompt，如果直接给文字这叫做













